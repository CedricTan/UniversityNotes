\documentclass[12pt, letterpaper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\hypersetup{
	colorlinks = true,
	linkcolor = black,
	urlcolor = blue
}

\setlength{\headheight}{15pt}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{\leftmark}
\lhead{EC220}

\title{
	{EC220 Introduction to Econometrics}\\
	{\large{Dr Canh T. Dang}}\\
	{\large{Lecture Notes}}
}
\author{Cedric Tan}
\date{September 2019}
\begin{document}
\maketitle
{\small
  \noindent\textbf{Causal Inference and Randomised Experiments}\\
  We begin Econometrics by understanding the difference between causality and correlation. We look into the different types of causality that can be applied to our econometric examples. We look at potential outcomes, counterfactuals and the issue of selection bias. Further, experiments are lightly covered based on observable and unobservable characteristics. We also look into the motivation for randomised experiments and how they help us reduce bias or conflict within our inferences. \hspace*{\fill}[1]

  \vspace{10pt}
  \noindent\textbf{Concept}\\
  Concept.\hspace*{\fill}[2]

\newpage
\tableofcontents
\newpage
\section{Introduction to Econometrics: Michaelmas Term}
The initial focus of MT is applied econometrics, particularly causal questions such as "what-if" questions. Mislabelling causality as correlation can be a critical error that people make when analysing data. The course intends to teach you how to analyse data and answer economic questions using "econometrics" and data.

\vspace{10pt}
\noindent Examples of what-ifs are below:
\begin{itemize}
	\item What happens to a country if it withdraws from a trade agreement?
	\item What is the effect of parents' education on children's education?
	\item What is the impact on your health if you go to a hospital?
\end{itemize}

\subsection{Causality}
\textbf{A causes B:} A contributes (or influences) to the occurrence of event B. The \textit{cause} A is partly responsible for the \textit{effect}
 B, and the effect B is partly dependent on the cause A. A can be necessary for the occurrence of B, but A can simply lead to fluctuations in B, this is still a causal relationship.
 
\vspace{10pt}
\noindent So we can take two definitions for causality:
\begin{itemize}
	\item A is a necessary condition for B to occur
	\item A can cause fluctuations in B
\end{itemize}
Labels are also necessary for the structure of causation:
\begin{itemize}
	\item Event A is called Treatment
	\item Event B is called the Outcome
	\item A third variable that causes the two events to happen is called a Confounder
\end{itemize}

\noindent We can have reverse causality - A causes B but also B causes A. An example would be \textit{Umbrellas} and \textit{Rain} where bringing umbrellas is caused by the possibility of rain. Sometimes timing helps to establish causality - as \textit{Rain} happens before \textit{Umbrellas}, \textit{Umbrellas} cannot cause \textit{Rain}.

\vspace{10pt}
\noindent Reasons that A and B are correlated:
\begin{enumerate}
	\item A causes B (direct causation)
	\item B causes A (reverse causation)
	\item A and B are consequences of a common cause but do not cause each other (confounder)
	\item A causes B and B causes A (bidirectional causation)
	\item A causes C which then causes B (indirect causation)
	\item No connection between A and B, the correlation is pure coincidence
\end{enumerate}
All statistical techniques only establish associations, causation requires interpretation.\\
Correlation: the extent to which A and B tend to decrease and increase at the same time.

\vspace{10pt}
\noindent Causation can occur without correlation, here is an example for medicine:\\
Illness (A) can cause death (B), but nowadays healthcare (C) can eliminate the correlation between common illness and death.

\subsection{Why Causality?}
This is the economist's comparative advantage, the ability to infer causality from correlation.

\vspace{10pt}
\noindent Examples of causality are listed below with classifications like the above.
\begin{itemize}
	\item Direct Causation:
	\item Reverse Causation:
	\item Confounder Problem:
	\item Bidirectional Causation:
	\item Indirect Causation
	\item Pure Coincidence:
\end{itemize}

\section{Counterfactuals}
Insured or not insured. Here we will see the use of counterfactuals to infer causality.

\subsection{Health Insurance}
What is the effect of health insurance on health? Health insurnace started as a voluntary scheme. The Obama "Affordable Care Act": attempted to compel the whole of America to buy health insurance. Similarly, is the NHS a good policy?

% Reference Mastering Metrics here - discussion on NHIS

\textbf{Question:} What is the effect of health insurance on health expenditures and on health outcomes?

\subsubsection{Health Insurance: Causal Questions}
We want to compare:
\begin{itemize}
	\item The health of someone with insurance $Y_i^T$
	\item The health of the same person without insurance $Y_i^C$
\end{itemize}
Though this is not a realistic possibility simply because we cannot have the exact same person simultaneously adopting both approaches. Although the data might show that there is a correlation between Health Insurance and Health, there could be other factors. For example, the United States spends more of its GDP on health care than do other developed nations, yet Americans are surprisingly unhealthy. However, America is also unusual in that is has no universal health insurance scheme. Perhaps there is a causal connection here.
\begin{itemize}
	\item The causal effect of insurance: heaving health insurance may lead to better health because of better health care
	\item The reverse causal effect: the less healthy are more likely to buy insurance
	\item Confounder effect: the more educated tend to buy insurance more often and they know how to live healthier (possibly)
	\item Pure coincidence: another possibility
\end{itemize}
Many of the working, prime-age poor, however, have long been uninsured. In fact, many uninsured Americans have chosen not to participate in an employer-provided insurance plans. Using the National Health Interview Survey (NHIS), that rates health from poor (1) to excellent (5), we can gauge, with characteristics and analysis of whether or not someone has health insurance or not, the possibility of a causal effect.

\vspace{10pt}
\noindent Terminology to take into account when looking into this case study:
\begin{itemize}
	\item Health insurance coverage for individual $i$ is described by a binary random variable; we call this the \textit{treatment}
	\begin{center}
	$D_i = {0,1}$
	\end{center}
	\item The outcome of interest, a measure of health status denoted by $Y_i$ and is the index proposed by the NHIS i.e. on a scale from 1 to 5
	\item Potential outcomes: describes what would have happened to someone under the scenarios where they had or had not been insured. 
	\item Those with insurance are called the \textit{treatment group}
	\item Those without insurance are called the \textit{control group}; a good control group reveals the fate of the treated in a counterfactual world where they are not treated
\end{itemize}

\subsubsection{Fruitless and Fruitful comparisons}
The critical thing to be mindful of is keeping things equal i.e. \textit{ceteris paribus}. Comparisons of people with and without health insurance are not apples to apples; such contrasts are apples to oranges, or worse. Among other differences, those with health insurance are better educated, have higher income, and are more likely to be working than the uninsured. Many of the differences in characteristics between the two classes are large and most are statistically precise enough to rule out the hypothesis that these discrepancies are merely chance findings. Thus it won't be surprising that most variables listed are high correlated with health as well as with health insurance status. Some other highly associated characteristics
\begin{itemize}
	\item Education
	\item Family income
	\item Age
	\item Employed vs Unemployed (less powerful indicator)
\end{itemize}
We use the Robert Frost metaphor for understanding the potential outcomes.
\begin{itemize}
		\item For everybody there are two potential outcomes \textbf{but only one is actually observed}:
	\begin{center}
		\[ Counterfactual =
		\begin{cases}
		Y_{1i} & if \; D_i = 0\\
		Y_{0i} & if \; D_i = 1
		\end{cases}
		\]
	\end{center}
	\item The treatment effect is the difference between the actual outcome and the counterfactual given the person has insurance. Thus the effect of having insurance for individual $i$ is $Y_{1i} - Y_{0i}$
\end{itemize}
To cement this down even further, lets consider the story of two students (K and M) going to MIT who have the option to subscribe to the MIT health insurance plan. Upon reflection, K decides that the MIT insurance is worth paying for since he fears he might get sick. Let's say that for K $Y_{0i} = 3$ and $Y_{1i} = 4$ for $i = K$ For K, the causal effect of insurance is one step up on the NHIS scale:
\begin{center}
	$Y_{1,K} - Y_{0,K} = 1$
\end{center}
M is also coming to MIT but does not fear sickness whatsoever. Not concerned about the winters in Boston, M does not think she will fall sick easily so opts not to buy the MIT insurance. Because $Y_{0,M} = Y_{1,M} = 1$ we have:
\begin{center}
	$Y_{1,M} - Y_{0,M} = 0$
\end{center}
We can summarise this data into a table below:
\begin{center}
	\begin{tabular}{ccc}
	\hline
	\hline
	& K & M\\
	\hline
	Potential outcome without insurance: $Y_{0i}$ & 3 & 5\\
	Potential outcome with insurance: $Y_{1i}$ & 4 & 5\\
	Treatment (insurance status chosen): $D_i$ & 1 & 0\\
	Actual health outcome: $Y_i$ & 4 & 5\\
	Treatment effect: $Y_{1i} - Y_{0i}$ & 1 & 0\\
	\hline 
	\end{tabular}
\end{center}
There is a selection problem for K and M. This is because:
\begin{itemize}
	\item We have: 
	\begin{center}
	$E[Y_i|D_i=1]-E[Y_i|D_i=0]$ 
	\end{center}
	Which we can call the observed difference in average health.
	\item This is equivalent to:
	\begin{center}
	$E[Y_{1i}|D_i=1]-E[Y_{0i}|D_i=0]$ (Average treatment effect on the treated) $+$ $E[Y_{0i}|D_i=1]-E[Y_{0i}|D_i=0]$ (Selection bias)
	\end{center}
	Which we can call the average treatment effect on the treated plus the selection bias
\end{itemize}
If we look at K and M again and compare the two outcomes they have given themselves, based on their decision to buy or not to buy insurance, we can see that K has a NHIS rating of $Y_K = Y_{1,K} = 4$ while M has a NHIS rating of $Y_M = Y_{0,M} = 5$. Taking the difference between them we obtain:
\begin{center}
	$Y_K - Y_M = -1$
\end{center}
From this, we might believe that K's decision to purchase insurance is counter-productive since M, even without purchasing insurance, has a higher NHIS rating than K. In fact, the comparison between frail K and hearty M tells us little about the causal effects of their choices. This can be seen by linking observed and potential outcomes as follows:
\begin{center}
	$Y_K - Y_M = Y_{1,K} - Y_{0,M} = [(Y_{1,K} - Y_{0,K}) + (Y_{0,K} - Y_{0,M})]$
\end{center}
The second part of the equation (in square brackets) is derived byu adding and subtracting $Y_{0,K}$, thereby generating two hidden comparisons that determine the one we see. The first comparison is the causal effect of health insurance on K, which is equal to $1$. The second is the difference between the two student's health status were both to decide insurance which is equal to $-2$. This $-2$ reflects K's relative frailty. Therefore, in the context of our effort to uncover causal effects we find the lack of comparability captured by the second term, as mentioned above, which is the \textbf{selection bias}.

\newpage
\section{Introduction to Experiments}
How do we solve the selection bias? One method is through random assignment of the treatment factor. That is to say that if $D_i$ is randomly assigned, it is (statistically) independent of potential outcomes such that there is no difference between $E[Y_{0i}|D_{i}=1]$ and $E[Y_{0i}|D_{i}=0]$. That means we eliminate out selection bias factor $E[Y_{0i}|D_i=1]-E[Y_{0i}|D_i=0]=0$.

\subsection{Developing the experiment}
Remember K and M from the MIT observation. If we were to randomly assign treatment in this case, the sample size would be too small. Further, characteristics between K and M would be too different and we would not be able to check for balance.

\vspace{10pt}
\noindent Thus, we can use the law of large numbers (LLN) to get a larger sample to test on. The theorem states:
\begin{center}
	\textbf{The larger the sample, the closer the sample average will become to the population mean. By making the sample large enough, sample statistics and population can be brought as close to what we want.}
\end{center}
The mathematical expectation is such that:
\begin{center}
\textit{The mathematical expectation of a variable, $Y_i$, is written $E[Y_i]$, is the population average of this variable. If $Y_i$ is a variable generated by a random process, such as throwing a die, $E[Y_i]$ is the average in infinitely many repetitions of this process. If $Y_i$ is a variable that comes from a sample survey, $E[Y_i]$ is the average obtained if everyone in the population from which the sample is drawn were to be enumerated.}
\end{center}
By having a large sample, we can wash out the random characteristics within our sample size making our experiment more indicative. Our aim is to remove the selection bias when inferring a causal effect, thus, with enough subjects, we can assure balance across the groups. Balance means no \textit{systematic differences} in characteristics e.g. we hold a similar fraction of men and women in each group. We can balance for observed characteristics but we can only assume randomisation would work for unobserved characteristics, such as motivations, incentives and inner behaviours.

\subsection{Randomized Trials}
Experimental random assignment eliminates selection bias. The logistics of a randomized experiment, sometimes called a randomized trial, can be complex, but the logic is simple. Taking note of our previous issue of characteristics between K and M, random assignment, with the LLN, makes this comparison \textit{ceteris paribus}.

\vspace{10pt}
\noindent In randomized trials, experimental samples are created by sampling from a population we'd like to study rather than by repeating a game, but the LLN works just the same. When sampled subjects are randomly divided (as if by a coin toss) into treatment and control groups, they come from the same underlying population. the LLN therefore promises that those in randomly assigned treatment and control samples will be similar if the samples are large enough. Randomly assigned groups should be similar in every way, including in ways that we cannot easily measure or observe. This is the approach to eliminating selection bias.

\vspace{10pt}
\noindent We can describe the power of random assignment using this conditional expectation:
\begin{center}
	\textit{The conditional expectation of a variable, $Y_i$, is given a dummy variable, $D_i = 1$, is written $E[Y_i|D_i = 1]$. This is the average of $Y_i$ in the population that has $D_i$ equal to $1$. Likewise, the conditional expectation of a variable, $Y_i$, given $D_i = 0$, written $E[Y_i|D_i = 0]$, is the average of $Y_i$ in the population that has $D_i$ equal to $0$. If $Y_i$ and $D_i$ are variables generated by a random process, such as throwing a die under different circumstances, $E[Y_i|D_i = d]$ is the average of infinitely many repetitions of this process while holding the circumstances indicated by $D_i$ at $D$. If $Y_i$ and $D_i$ come from a sample survey, $E[Y_i|D_i = d]$ is the average computed when everyone in the population who has $D_i = d$ is sampled.}
\end{center}
Because randomly assigned treatment and control groups come from the same underlying population, they are the same in every way, including their expected $Y_{0i}$. In other words, the conditional expectations $E[Y_{0i}|D_{i} = 1]$ and $E[Y_{0i}|D_{i} = 0]$ are the same. This means that when $D_i$ is randomly assigned, $E[Y_{0i}|D_i = 1] = E[Y_{0i}|D_{i} = 0]$ and the difference in expectations by treatment status captures the causal effect of treatment:
\begin{center}	
	$E[Y_i|D_i = 1] - E[Y_i|D_i = 0]$\\
	$= E[Y_{1i}|D_i = 1] - E[Y_{0i}|D_i = 0]$\\
	$= E[Y_{0i}+ \kappa |D_i = 1] - E[Y_{0i}|D_i = 0]$\\
	$= \kappa + E[Y_{0i}|D_i = 1] - E[Y_{0i}|D_i = 0]$\\
	$= \kappa $; which is the average causal effect of insurance on health in this case.
\end{center}

\subsection{Results and Replication}
The replication of the results provided a much richer interpretation of a health insurance policy that potentially affects millions of people. Below are some reasons for differences in replication between the RAND and Oregon Trail experiments:
\begin{itemize}
	\item Economics is not physics: there are no causal relationships fixed across time and space
	\item There were different set ups: RAND - with its varying generosity, Oregon trial - insured to uninsured
	\item There were different populations
	\item Medicine changed between 1974 and 2008
\end{itemize}
Similar results from multiple experiments for related treatments and related population of interests boosts our confidence about the external validity i.e. the transferability of the theory, of our result. The same goes for non-experimental (observational) methods.

\vspace{10pt}
\noindent Sometimes experiments simply cannot be done. Here are some reasons why:
\begin{itemize}
	\item Ethical issues
	\item Budgets
	\item Inability to experiment with things that have already happened e.g. Fiscal Policy
\end{itemize}
But experiments are constantly occurring within business and government. For example, Google Adwords will randomise adverts to see clicks or compare search results or search trends with marketing targets.

\vspace{10pt}
\noindent Even if we can't do an experiment, it is useful to think about one i.e. experiments which can be a benchmark for observational methods. Here, we can mimic the ideal experiment through a comparison. However, the experiment is not a silver bullet, there are questions that cannot be answered. For example, events that have happened once and cannot be redone again such as Brexit or the 2007 financial crisis. Although there are similar scenarios, we cannot say that it is the same situation.

\newpage
\section{Experimental Approaches}

\subsection{Sampling}
When estimating the treatment effect or checking for balance, how can we tell whether a difference in the means did not arise by chance? There are serveral sources for variability in our estimates:
\begin{itemize}
	\item Sample selection: 
	\item Statistical modelling: our choice of estimators differs
	\item Imprecision in measurement: the information is not precisely measured
	\item Sampling: variability caused by different samples from the population
\end{itemize}
Here, we focus on how sampling can affect the means in this lecture.

\subsubsection{Populations and Samples}
We try to analyse a representative sample rather than a whole population. Below are a few terms to take note of:
\begin{itemize}
	\item Population mean: the expectation of the population distribution of the Data $Y_i$ or $\mu$. This is a parameter (a fixed number). $\mu = E[Y_i]$
	\item We would like to also \textit{estimate} this parameter. The sample average:
	\begin{center}
		$$\overline{Y}_n = Avg_n(Y_i) = \frac{1}{n}\sum^n_{i=1}Y_i$$
	\end{center}
	 is an estimator of the population mean. An estimator is an example of a sample statistic, a function of sample data used to \textit{infer} the parameter.
	\item An \textit{estimate} is the value the estimator takes on for a particular sample
	\item Sampling variability is about the size of the differences between: $\overline{Y}_n - \mu$
\end{itemize}

\subsubsection{Unbiasedness}
If the estimator is unbiased, we must have $E[\overline{Y}_n] = E[Y_i] = \mu$. This is a very desirable property. Unbiasedness means that in many repeated random samples, the average of the sample averages will equate the population mean. The sample average does not deviate from the mean in any direction, on average. Unbiasedness must hold for samples of any size.

\vspace{10pt}
\noindent We want to assess the size of $\overline{Y}_n - \mu$. Unbiasedness means that:
\begin{center}
	$E[\overline{Y}_n - \mu] = 0$
\end{center}
We are also interested in the absolute values of these differences, so we typically look at the squared error terms otherwise we will bear no good result since the expected value, not squared, will average out to 0. Hence we have the equation: $E[\overline{Y}_n - \mu]^2 = 0$

\vspace{10pt}
\noindent For any random variable $Y_i$, we define variance as:
\begin{center}
	$Var(Y_i) = E[\overline{Y}_n -\mu]^2$
\end{center}
Conceptually, $E[\overline{Y}_n - E[\overline{Y}_n]]^2$ is the variance of the sample average we would obtain if we drew (infinitely) many samples. If the estimator is unbiased we will have:
\begin{center}
	$E[\overline{Y}_n - E[\overline{Y}_n]]^2 = E[\overline{Y}_n -\mu]^2$
\end{center}

Variance of the sample average:
\begin{center}
	$$Var(\overline{Y}_n) = Var( \frac{1}{n	} \sum_{i=n}^{n} Y_i)$$
\end{center}

\end{document}